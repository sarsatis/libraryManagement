foreach ($baseline in $baselines) {

    Write-Host "Processing baseline: $baseline" -ForegroundColor Magenta

    $filename = "${baseline}_$currentdate"
    $csvfilepath = ".\sourcefiles\$filename.csv"
    $jsonfilepath = ".\sourcefiles\$filename.json"

    if (Test-Path $csvfilepath) { 
        Remove-Item -Path $csvfilepath -Force 
        Remove-Item -Path $jsonfilepath -Force
    }

    $allCsvLines = @()
    $csvHeader = "bunit,subscription,report_id,date,host_name,region,environment,platform,status,id,title,message"
    $allCsvLines += $csvHeader

    $totalVMs = 0  # Track number of VMs with data

    foreach ($sub in $subscriptions) {

        $VMquery = @"
guestconfigurationresources
| where subscriptionId == '$($sub.Id)'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains '$baseline'
| project subscriptionId, id, name, location, resources = properties.latestAssignmentReport.resources, vmid = split(properties.targetResourceId, '/')[(-1)],
reportid = split(properties.latestReportId, '/')[(-1)], reporttime = properties.lastComplianceStatusChecked
| order by id
"@

        $VMresults = Search-AzGraph -Query $VMquery
        if ($VMresults.count -eq 0) { continue }

        Write-Host "Querying subscription: $($sub.Name) ($($sub.Id)) — Total VMs: $($VMresults.count)" -ForegroundColor Cyan

        foreach ($vm in $VMresults) {
            Write-Host "Processing VM: $($vm.vmid)" -ForegroundColor Cyan

            $compliancequery = @"
guestconfigurationresources
| where id == '$($vm.id)'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains '$baseline'
| project subscriptionId, id, name, location, resources = properties.latestAssignmentReport.resources, vmid = split(properties.targetResourceId, '/')[(-1)],
reportid = split(properties.latestReportId, '/')[(-1)], reporttime = properties.lastComplianceStatusChecked
| order by id
| extend resources = iff(isnull(resources[0]), dynamic([{}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{}]), reasons)
| mv-expand reasons
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", ""),
    message = reasons.phrase
"@

            $batchsize = 1000
            $skip = 0
            $complianceResults = @()

            do {
                $pagedResults = if ($skip -eq 0) {
                    Search-AzGraph -Query $compliancequery -First $batchsize
                } else {
                    Search-AzGraph -Query $compliancequery -First $batchsize -Skip $skip
                }
                $complianceResults += $pagedResults
                $skip += $batchsize
            } while ($pagedResults.Count -eq $batchsize)

            if ($complianceResults.Count -gt 0) {
                $totalVMs += 1
            }

            foreach ($ctl in $complianceResults) {
                $resultline = "$($ctl.bunit),$($ctl.subscription),$($ctl.report_id),$($ctl.Date),$($ctl.host_name),$($ctl.region),$($ctl.environment),$($ctl.platform),$($ctl.status),$($ctl.id),$($ctl.title),$($ctl.message)"
                $allCsvLines += $resultline
            }
        }
    }

    # Write CSV + JSON
    Write-Host "Writing results to: $csvfilepath" -ForegroundColor Yellow
    $allCsvLines | Set-Content -Path $csvfilepath -Encoding UTF8

    Write-Host "Converting to JSON: $jsonfilepath" -ForegroundColor Yellow
    Import-Csv -Path $csvfilepath | ConvertTo-Json -Depth 10 | Set-Content -Path $jsonfilepath -Encoding UTF8

    Write-Host "Total VMs with compliance data for baseline $baseline: $totalVMs" -ForegroundColor Green
}

---
Subject: Request for GCS-Approved CIS Benchmark Document for Waivers Automation

Hi Prabhu,

Thank you again for taking the time to walk me through the consolidated document containing the GCS-approved CIS benchmark IDs across supported platforms. It was very helpful to see that the discrepancies I had previously observed—particularly with Amazon Linux 2023—are addressed clearly in this version.

As discussed, we are currently working on automating the validation of Amazon Inspector 2 scan results against GCS-approved CIS profiles. The goal is to programmatically mark any previously approved findings as “Skipped,” irrespective of their actual scan status, to improve the accuracy of compliance reporting.

To enable this functionality, we are generating a waivers.yaml file per platform, listing the GCS-approved CIS IDs. This file will be consumed by downstream exemption processing systems. For this to work reliably, we need access to a complete and consistent source of truth across all platforms—Amazon Linux 2/2023, Windows Server 2019/2022, Rocky Linux 8, RHEL 8.8/8.10, etc.

The document you showed during our call seems to be exactly what we need. I understand that it’s currently under review by the security team and cannot be shared without proper approvals. As you suggested, I’m formally requesting access to this document and copying my manager [Manager’s Full Name] to highlight the importance and urgency of this requirement.

This file is essential for ensuring that our exemption logic functions correctly and that compliance reporting remains consistent and auditable.

Please let us know if there's anything further you need from our side to help expedite the approval process.

Thanks again for your support.

Best regards,
Sarthak S
---
Subject: Request for GCS-Approved CIS Benchmark Document for Waivers Automation

Hi Prabhu,

Thank you again for walking me through the consolidated document containing the GCS-approved CIS benchmark IDs. It was helpful to see how the discrepancies we observed—especially with Amazon Linux 2023—have been addressed.

As discussed, we’re currently working on automating the validation of Amazon Inspector 2 scan results against GCS-approved CIS profiles. The goal is to programmatically mark any previously approved findings as “Skipped,” regardless of their Inspector 2 status, to ensure more accurate compliance reporting.

To support this, we are generating a waivers.yaml file per platform that lists all the GCS-approved CIS IDs. This file is critical to our exemption processing logic and directly impacts downstream systems.

The document you showed during our call appears to be exactly what we need. However, we still need to investigate further to ensure it includes coverage for all required platforms (Amazon Linux 2/2023, Windows Server 2019/2022, Rocky Linux 8, RHEL 8.8/8.10, etc.). For that analysis, we’ll need access to the full document.

As you mentioned, it’s currently pending security approvals for external sharing. Per your suggestion, I’m reaching out formally—and copying my manager [Manager’s Full Name]—to request access to this document. Having access will allow us to confirm platform coverage and ensure we’re working with a consistent and authoritative data source.

Please let us know if there’s anything further you need from our side to help move this forward with the security team.

Appreciate your support!

Best regards,
Sarthak S
[Your Job Title]
[Your Team or Department]
[Your Company Name]
---
import requests
import msal
import json
import csv
import os
from datetime import datetime
import urllib3

# --------- Disable SSL warnings if verification is disabled ---------
VERIFY_SSL = False
if not VERIFY_SSL:
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --------- Configurations ---------
TENANT_ID = "<NPE SP Tenant ID>"
CLIENT_ID = "<NPE SP App ID>"
CLIENT_SECRET = "<NPE SP App Secret>"

PROXIES = {
    # 'http': 'http://proxyaddress:port',
    # 'https': 'http://proxyaddress:port',
}

QUERY_TEMPLATE = """
guestconfigurationresources
| where subscriptionId == '{subscription_id}'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
| extend resources = iff(isnull(resources[0]), dynamic([{{}}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{{}}]), reasons)
| mv-expand reasons
| order by id
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"""

def log(msg):
    print(msg)

def get_access_token(tenant_id, client_id, client_secret):
    authority = f"https://login.microsoftonline.com/{tenant_id}"
    scope = ["https://management.azure.com/.default"]
    app = msal.ConfidentialClientApplication(
        client_id, authority=authority, client_credential=client_secret,
    )
    result = app.acquire_token_for_client(scopes=scope)
    if "access_token" in result:
        return result["access_token"]
    else:
        raise Exception(f"Failed to get token: {result.get('error_description')}")

def get_subscriptions(token):
    url = "https://management.azure.com/subscriptions?api-version=2020-01-01"
    headers = {"Authorization": f"Bearer {token}"}
    resp = requests.get(url, headers=headers, proxies=PROXIES, verify=VERIFY_SSL)
    resp.raise_for_status()
    subs = resp.json().get("value", [])
    return [(sub["subscriptionId"], sub["displayName"]) for sub in subs]

def run_resource_graph_query(token, subscription_id):
    url = "https://management.azure.com/providers/Microsoft.ResourceGraph/resources?api-version=2022-10-01"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    query = QUERY_TEMPLATE.format(subscription_id=subscription_id)
    all_results = []
    page = 1
    skip_token = None

    while True:
        body = {
            "subscriptions": [subscription_id],
            "query": query,
            "options": {
                "resultFormat": "objectArray"
            }
        }
        if skip_token:
            body["options"]["$skipToken"] = skip_token

        response = requests.post(url, headers=headers, json=body, proxies=PROXIES, verify=VERIFY_SSL)
        response.raise_for_status()
        result_json = response.json()

        data = result_json.get("data", [])
        all_results.extend(data)

        log(f"Page {page}: Fetched {len(data)} records (Total: {len(all_results)})")

        skip_token = result_json.get("$skipToken")
        if not skip_token:
            break
        page += 1

    return {"data": all_results}

# --------- Main Execution ---------
def main():
    log("Authenticating and acquiring token...")
    token = get_access_token(TENANT_ID, CLIENT_ID, CLIENT_SECRET)

    log("Retrieving Azure subscriptions...")
    subscriptions = get_subscriptions(token)
    log(f"Total subscriptions: {len(subscriptions)}")

    start_time = datetime.now()
    log(f"Start Time: {start_time}")

    current_date = start_time.strftime('%Y-%m-%d')
    filename = f"CIS_Benchmark_Windows2022_Baseline_1_0_{current_date}"
    os.makedirs("sourcefiles", exist_ok=True)
    csv_filepath = f"./sourcefiles/{filename}.csv"
    json_filepath = f"./sourcefiles/{filename}.json"

    headers = ["bunit", "subscription", "report_id", "Date", "host_name", "region",
               "environment", "platform", "status", "id", "title"]

    all_rows = []

    for sub_id, sub_name in subscriptions:
        log(f"\nProcessing subscription: {sub_name} ({sub_id})")
        result_json = run_resource_graph_query(token, sub_id)
        data = result_json.get("data", [])
        log(f"Records from {sub_name}: {len(data)}")
        all_rows.extend(data)

    if not all_rows:
        log("No compliance data found across subscriptions.")
        return

    log(f"\nWriting CSV to: {csv_filepath}")
    with open(csv_filepath, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for row in all_rows:
            filtered_row = {key: row.get(key, "") for key in headers}
            writer.writerow(filtered_row)

    log(f"Writing JSON to: {json_filepath}")
    with open(json_filepath, mode="w", encoding="utf-8") as f:
        json.dump(all_rows, f, indent=2)

    end_time = datetime.now()
    duration = end_time - start_time
    log(f"\nDone. Total records written: {len(all_rows)}")
    log(f"End Time: {end_time}")
    log(f"Total Execution Time: {duration}")

if __name__ == "__main__":
    main()



*************************************************************
import requests
import json
import os
from datetime import datetime
import pandas as pd

# ----------------- Configuration -----------------
TENANT_ID = "<NPE SP Tenant ID>"
CLIENT_ID = "<NPE SP App ID>"
CLIENT_SECRET = "<NPE SP Client Secret>"

PROXIES = {
    "http": "http://your-proxy-url:port",
    "https": "http://your-proxy-url:port"
}

VERIFY_SSL = False  # Disable SSL verification (NOT recommended for production)

QUERY_TEMPLATE = """
guestconfigurationresources
| where subscriptionId == '{}'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
| extend resources = iff(isnull(resources[0]), dynamic([{{}}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{{}}]), reasons)
| mv-expand reasons
| order by id
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"""

RESOURCE_GRAPH_API = "https://management.azure.com/providers/Microsoft.ResourceGraph/resources?api-version=2021-03-01"
SUBS_API = f"https://management.azure.com/subscriptions?api-version=2020-01-01"

# ----------------- Authenticate -----------------
def get_access_token():
    url = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
    data = {
        "grant_type": "client_credentials",
        "client_id": CLIENT_ID,
        "client_secret": CLIENT_SECRET,
        "scope": "https://management.azure.com/.default"
    }
    resp = requests.post(url, data=data, proxies=PROXIES, verify=VERIFY_SSL)
    resp.raise_for_status()
    token = resp.json()["access_token"]
    print("Authenticated successfully.")
    return token

# ----------------- Get Subscriptions -----------------
def get_subscriptions(token):
    headers = {"Authorization": f"Bearer {token}"}
    resp = requests.get(SUBS_API, headers=headers, proxies=PROXIES, verify=VERIFY_SSL)
    resp.raise_for_status()
    subs = resp.json().get("value", [])
    print(f"Fetched {len(subs)} subscriptions.")
    return subs

# ----------------- Run Resource Graph Query -----------------
def run_query(token, subscription_id, skip_token=None):
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    query = QUERY_TEMPLATE.format(subscription_id)
    payload = {
        "query": query,
        "subscriptions": [subscription_id],
        "options": {
            "resultFormat": "objectArray"
        }
    }
    if skip_token:
        payload["skipToken"] = skip_token

    resp = requests.post(RESOURCE_GRAPH_API, headers=headers, json=payload, proxies=PROXIES, verify=VERIFY_SSL)
    resp.raise_for_status()
    return resp.json()

# ----------------- Main Script -----------------
def main():
    start_time = datetime.now()
    print(f"Start Time: {start_time}")

    filename = f"CIS_Benchmark_Windows2022_Baseline_1_0_PYTHON_{start_time.strftime('%Y-%m-%d')}"
    os.makedirs("sourcefiles", exist_ok=True)
    csv_filepath = f"./sourcefiles/{filename}.csv"
    json_filepath = f"./sourcefiles/{filename}.json"

    for f in [csv_filepath, json_filepath]:
        if os.path.exists(f):
            os.remove(f)

    token = get_access_token()
    subscriptions = get_subscriptions(token)

    all_rows = []
    for sub in subscriptions:
        sub_id = sub["subscriptionId"] if "subscriptionId" in sub else sub["id"]
        print(f"Querying subscription: {sub_id}")

        skip_token = None
        total_rows_sub = 0

        while True:
            result = run_query(token, sub_id, skip_token)
            data = result.get("data", [])
            total_rows_sub += len(data)
            print(f"Fetched {len(data)} rows from subscription {sub_id} (skipToken={skip_token})")

            all_rows.extend(data)

            skip_token = result.get("skipToken")
            if not skip_token:
                break

        print(f"Total rows fetched for subscription {sub_id}: {total_rows_sub}")

    if not all_rows:
        print("No data fetched.")
        return

    # Save CSV
    df = pd.DataFrame(all_rows)
    df.to_csv(csv_filepath, index=False)
    print(f"Saved CSV to {csv_filepath}")

    # Save JSON
    with open(json_filepath, "w", encoding="utf-8") as f:
        json.dump(all_rows, f, indent=2)
    print(f"Saved JSON to {json_filepath}")

    end_time = datetime.now()
    duration = end_time - start_time
    print(f"End Time: {end_time}")
    print(f"Total Duration: {duration}")

if __name__ == "__main__":
    main()


----
import subprocess
import json
import os
import csv
from datetime import datetime

# -------------------- Configuration --------------------
TENANT_ID = "<NPE SP Tenant ID>"
APP_ID = "<NPE SP App ID>"
PASSWORD = ""  # Securely retrieved/stored SP password

# -------------------- Login --------------------
print("Logging into Azure...")
subprocess.run([
    "az.cmd", "login",
    "--service-principal",
    "--username", APP_ID,
    "--password", PASSWORD,
    "--tenant", TENANT_ID
], check=True)

# -------------------- Setup --------------------
start_time = datetime.now()
print(f"Start Time: {start_time}")

filename = f"CIS_Benchmark_Windows2022_Baseline_1_0_PYTHON_{start_time.strftime('%Y-%m-%d')}"
csv_filepath = f"./sourcefiles/{filename}.csv"
json_filepath = f"./sourcefiles/{filename}.json"
os.makedirs("sourcefiles", exist_ok=True)

# Clean up old files
for path in [csv_filepath, json_filepath]:
    if os.path.exists(path):
        os.remove(path)

header = [
    "bunit", "subscription", "report_id", "Date", "host_name", "region",
    "environment", "platform", "status", "id", "title"
]
all_data = []

# -------------------- Get Subscriptions --------------------
subs_raw = subprocess.check_output(["az.cmd", "account", "list", "--query", "[].{id:id, name:name}"])
subscriptions = json.loads(subs_raw)
print(f"Total subscriptions: {len(subscriptions)}")

# -------------------- Query per Subscription --------------------
def run_az_graph_query(subscription_id, first=1000, skip=0):
    query = f"""
    guestconfigurationresources
    | where subscriptionId == '{subscription_id}'
    | where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
    | where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
    | project 
        bunit = "azure", 
        subscription = subscriptionId, 
        report_id = split(properties.latestReportId, '/')[(-1)], 
        Date = format_datetime(todatetime(properties.lastComplianceStatusChecked), "yyyy-MM-dd"), 
        host_name = split(properties.targetResourceId, '/')[(-1)],
        region = location, 
        environment = case(
            tolower(substring(host_name, 5, 1)) == "d", "dev",
            tolower(substring(host_name, 5, 1)) == "q", "qa",
            tolower(substring(host_name, 5, 1)) == "u", "uat",
            tolower(substring(host_name, 5, 1)) == "p", "prod",
            "UNKNOWN"
        ),
        platform = split(name, "_")[2], 
        status = iif(
            array_length(properties.latestAssignmentReport.resources) == 0, "skipped",
            iif(properties.latestAssignmentReport.resources[0].complianceStatus == "true", "passed", "failed")
        ),
        id = split(properties.latestAssignmentReport.resources[0].resourceID, "_")[3], 
        title = replace_string(tostring(properties.latestAssignmentReport.resources[0].resourceId), "[WindowsControlTranslation]", "")
    """

    cmd = [
        "az.cmd", "graph", "query",
        "--query", query,
        "--first", str(first),
        "--skip", str(skip),
        "--output", "json"
    ]

    proc = subprocess.run(cmd, capture_output=True, text=True)
    if proc.returncode != 0:
        print(f"Error querying subscription {subscription_id}:", proc.stderr)
        return None

    return json.loads(proc.stdout)

for sub in subscriptions:
    sub_id = sub['id']
    sub_name = sub['name']
    print(f"\nProcessing subscription: {sub_name} ({sub_id})")

    skip = 0
    batch_size = 1000

    while True:
        print(f"Fetching rows, skip={skip}")
        result = run_az_graph_query(sub_id, first=batch_size, skip=skip)
        if not result or 'data' not in result or not result['data']['rows']:
            print("No more data or error.")
            break

        columns = [col['name'] for col in result['data']['columns']]
        rows = result['data']['rows']

        # Convert rows to dicts and append
        batch_data = [dict(zip(columns, row)) for row in rows]
        all_data.extend(batch_data)

        print(f"Fetched {len(rows)} rows from subscription {sub_name}")

        if len(rows) < batch_size:
            # Last batch for this subscription
            break

        skip += batch_size

print(f"\nTotal records fetched: {len(all_data)}")

# -------------------- Write CSV --------------------
with open(csv_filepath, mode='w', newline='', encoding='utf-8') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=header)
    writer.writeheader()
    for row in all_data:
        writer.writerow(row)

print(f"CSV data saved to {csv_filepath}")

# -------------------- Write JSON --------------------
with open(json_filepath, mode='w', encoding='utf-8') as jsonfile:
    json.dump(all_data, jsonfile, indent=2)

print(f"JSON data saved to {json_filepath}")

# -------------------- End Time --------------------
end_time = datetime.now()
duration = end_time - start_time
print(f"\nEnd Time: {end_time}")
print(f"Total Execution Time: {duration}")




******************************************************************************************
# -------------------- Secure Login --------------------
$SecurePassword = ConvertTo-SecureString -String "" -AsPlainText -Force
$TenantId = "<NPE SP Tenant ID>"
$ApplicationId = "<NPE SP App ID>"
$Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecurePassword
Connect-AzAccount -ServicePrincipal -TenantId $TenantId -Credential $Credential -WarningAction Ignore

# -------------------- Start Timer --------------------
$startTime = Get-Date
Write-Host "Start Time: $startTime" -ForegroundColor Yellow

# -------------------- Prepare --------------------
$subscriptions = Get-AzSubscription
Write-Host "Total subscriptions: $($subscriptions.count)" -ForegroundColor Cyan

$currentdate = Get-Date -Format "yyyy-MM-dd"
$filename= "CIS_Benchmark_Windows2022_Baseline_1_0_$currentdate"
$csvfilepath = ".\sourcefiles\$filename.csv"
$jsonfilepath = ".\sourcefiles\$filename.json"

if (Test-Path $csvfilepath) { 
    Remove-Item -Path $csvfilepath -Force 
    Remove-Item -Path $jsonfilepath -Force
}

# Initialize array to hold CSV lines and add header
$allCsvLines = @()
$csvHeader = "bunit,subscription,report_id,date,host_name,region,environment,platform,status,id,title"
$allCsvLines += $csvHeader

# -------------------- Loop through subscriptions --------------------
foreach ($sub in $subscriptions) {

    # Query VMs in this subscription (no pagination needed here assuming smaller result)
    $VMquery = @"
guestconfigurationresources
| where subscriptionId == '$($sub.Id)'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project subscriptionId, id, name, location, resources = properties.latestAssignmentReport.resources, vmid = split(properties.targetResourceId, '/')[(-1)],
reportid = split(properties.latestReportId, '/')[(-1)], reporttime = properties.lastComplianceStatusChecked
| order by id
"@

    $VMresults = Search-AzGraph -Query $VMquery
    if ($VMresults.count -gt 0) {
        Write-Host "Querying subscription Name: $($sub.Name), ID: $($sub.Id)" -ForegroundColor Cyan
        Write-Host "Total VMs in subscription $($sub.Name): $($VMresults.count)" -ForegroundColor Cyan
    } else {
        continue
    }

    # -------------------- Loop through each VM --------------------
    foreach ($vm in $VMresults) {
        Write-Host "Querying VM: $($vm.vmid)" -ForegroundColor Cyan
        
        $compliancequery = @"
guestconfigurationresources
| where id == '$($vm.id)'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project subscriptionId, id, name, location, resources = properties.latestAssignmentReport.resources, vmid = split(properties.targetResourceId, '/')[(-1)],
reportid = split(properties.latestReportId, '/')[(-1)], reporttime = properties.lastComplianceStatusChecked
| order by id
| extend resources = iff(isnull(resources[0]), dynamic([{}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{}]), reasons)
| mv-expand reasons
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"@

        # -------------------- Pagination for compliance query --------------------
        $batchsize = 1000
        $skip = 0
        $complianceResults = @()

        do {
            if ($skip -eq 0) {
                $pagedResults = Search-AzGraph -Query $compliancequery -First $batchsize
            } else {
                $pagedResults = Search-AzGraph -Query $compliancequery -First $batchsize -Skip $skip
            }

            $complianceResults += $pagedResults
            $skip += $batchsize
        } while ($pagedResults.Count -eq $batchsize)

        Write-Host "Total row count for VM $($vm.vmid): $($complianceResults.Count)" -ForegroundColor Cyan

        # -------------------- Collect all compliance results for batch write --------------------
        foreach ($ctl in $complianceResults) {
            $resultline = "$($ctl.bunit),$($ctl.subscription),$($ctl.report_id),$($ctl.Date),$($ctl.host_name),$($ctl.region),$($ctl.environment),$($ctl.platform),$($ctl.status),$($ctl.id),$($ctl.title)"
            $allCsvLines += $resultline
        }
    }
}

# -------------------- Write all results once --------------------
Write-Host "Writing all results to CSV file..." -ForegroundColor Yellow
$allCsvLines | Set-Content -Path $csvfilepath -Encoding UTF8

# -------------------- Convert to JSON --------------------
Write-Host "Converting CSV to JSON..." -ForegroundColor Yellow
Import-Csv -Path $csvfilepath | ConvertTo-Json -Depth 10 | Set-Content -Path $jsonfilepath -Encoding UTF8

# -------------------- End Timer --------------------
$endTime = Get-Date
$duration = $endTime - $startTime
Write-Host "End Time: $endTime" -ForegroundColor Yellow
Write-Host "Total Execution Time: $($duration.ToString())" -ForegroundColor Green



************************************************************************************************
# -------------------- Secure Login --------------------
$SecurePassword = ConvertTo-SecureString -String "" -AsPlainText -Force
$TenantId = "<NPE SP Tenant ID>"
$ApplicationId = "<NPE SP App ID>"
$Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecurePassword
Connect-AzAccount -ServicePrincipal -TenantId $TenantId -Credential $Credential -WarningAction Ignore

# -------------------- Start Timer --------------------
$startTime = Get-Date
Write-Host "Start Time: $startTime" -ForegroundColor Yellow

# -------------------- File Setup --------------------
$subscriptions = Get-AzSubscription
Write-Host "Total subscriptions: $($subscriptions.Count)" -ForegroundColor Cyan

$currentDate = Get-Date -Format "yyyy-MM-dd"
$filename = "CIS_Benchmark_Windows2022_Baseline_1_0_$currentDate"
$csvFilePath = ".\sourcefiles\$filename.csv"
$jsonFilePath = ".\sourcefiles\$filename.json"

Remove-Item -Path $csvFilePath -Force -ErrorAction SilentlyContinue
Remove-Item -Path $jsonFilePath -Force -ErrorAction SilentlyContinue

# Initialize list to accumulate all CSV lines
$allCsvLines = @()

# Add CSV header
$csvHeader = "bunit,subscription,report_id,Date,host_name,region,environment,platform,status,id,title"
$allCsvLines += $csvHeader

# -------------------- Subscription Loop --------------------
foreach ($sub in $subscriptions) {
    $subId = $sub.Id
    $subName = $sub.Name

    $complianceQuery = @"
guestconfigurationresources
| where subscriptionId == '$subId'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
| extend resources = iff(isnull(resources[0]), dynamic([{}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{}]), reasons)
| mv-expand reasons
| order by id
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"@

    # Pagination setup
    $batchSize = 1000
    $skip = 0
    $allResults = @()

    do {
        if ($skip -eq 0) {
            $pagedResults = Search-AzGraph -Query $complianceQuery -First $batchSize
        } else {
            $pagedResults = Search-AzGraph -Query $complianceQuery -First $batchSize -Skip $skip
        }

        $allResults += $pagedResults
        $skip += $batchSize
    } while ($pagedResults.Count -eq $batchSize)

    if ($allResults.Count -gt 0) {
        Write-Host "Processing subscription: $subName ($subId)" -ForegroundColor Cyan
        Write-Host "Total row count: $($allResults.Count)" -ForegroundColor Cyan

        foreach ($r in $allResults) {
            $line = "$($r.bunit),$($r.subscription),$($r.report_id),$($r.Date),$($r.host_name),$($r.region),$($r.environment),$($r.platform),$($r.status),$($r.id),$($r.title)"
            $allCsvLines += $line
        }
    }
}

# Write all lines at once to CSV
Write-Host "Writing all results to CSV file..." -ForegroundColor Yellow
$allCsvLines | Set-Content -Path $csvFilePath -Encoding UTF8

# -------------------- Export to JSON --------------------
Write-Host "Converting CSV to JSON..." -ForegroundColor Yellow
Import-Csv -Path $csvFilePath | ConvertTo-Json -Depth 10 | Set-Content -Path $jsonFilePath -Encoding UTF8

# -------------------- End Timer --------------------
$endTime = Get-Date
$duration = $endTime - $startTime
Write-Host "End Time: $endTime" -ForegroundColor Yellow
Write-Host "Total Execution Time: $($duration.ToString())" -ForegroundColor Green




***************************************************************************************************************************


# -------------------- Secure Login --------------------
$SecurePassword = ConvertTo-SecureString -String "" -AsPlainText -Force
$TenantId = "<NPE SP Tenant ID>"
$ApplicationId = "<NPE SP App ID>"
$Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecurePassword
Connect-AzAccount -ServicePrincipal -TenantId $TenantId -Credential $Credential -WarningAction Ignore

# -------------------- Start Timer --------------------
$startTime = Get-Date
Write-Host "Start Time: $startTime" -ForegroundColor Yellow

# -------------------- File Setup --------------------
$subscriptions = Get-AzSubscription
Write-Host "Total subscriptions: $($subscriptions.Count)" -ForegroundColor Cyan

$currentDate = Get-Date -Format "yyyy-MM-dd"
$filename = "CIS_Benchmark_Windows2022_Baseline_1_0_$currentDate"
$csvFilePath = ".\sourcefiles\$filename.csv"
$jsonFilePath = ".\sourcefiles\$filename.json"

Remove-Item -Path $csvFilePath -Force -ErrorAction SilentlyContinue
Remove-Item -Path $jsonFilePath -Force -ErrorAction SilentlyContinue

$csvHeader = "bunit,subscription,report_id,Date,host_name,region,environment,platform,status,id,title"
Add-Content -Path $csvFilePath -Value $csvHeader

# -------------------- Subscription Loop --------------------
foreach ($sub in $subscriptions) {
    $subId = $sub.Id
    $subName = $sub.Name

    $complianceQuery = @"
guestconfigurationresources
| where subscriptionId == '$subId'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
| extend resources = iff(isnull(resources[0]), dynamic([{}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{}]), reasons)
| mv-expand reasons
| order by id
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    Date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"@

    # Pagination setup
    $batchSize = 1000
    $skip = 0
    $allResults = @()

    do {
        $pagedResults = Search-AzGraph -Query $complianceQuery -First $batchSize -Skip $skip
        $allResults += $pagedResults
        $skip += $batchSize
    } while ($pagedResults.Count -eq $batchSize)

    if ($allResults.Count -gt 0) {
        Write-Host "Processing subscription: $subName ($subId)" -ForegroundColor Cyan
        Write-Host "Total row count: $($allResults.Count)" -ForegroundColor Cyan

        foreach ($r in $allResults) {
            $line = "$($r.bunit),$($r.subscription),$($r.report_id),$($r.Date),$($r.host_name),$($r.region),$($r.environment),$($r.platform),$($r.status),$($r.id),$($r.title)"
            Add-Content -Path $csvFilePath -Value $line
        }
    }
}

# -------------------- Export to JSON --------------------
Write-Host "All subscriptions processed"
Write-Host "Converting CSV to JSON..."
Import-Csv -Path $csvFilePath | ConvertTo-Json -Depth 10 | Set-Content -Path $jsonFilePath -Encoding utf8

# -------------------- End Timer --------------------
$endTime = Get-Date
$duration = $endTime - $startTime
Write-Host "End Time: $endTime" -ForegroundColor Yellow
Write-Host "Total Execution Time: $($duration.ToString())" -ForegroundColor Green


---
# Secure login setup
$SecurePassword = ConvertTo-SecureString -String "" -AsPlainText -Force
$TenantId = "<NPE SP Tenant ID>"
$ApplicationId = "<NPE SP App ID>"
$Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecurePassword

Connect-AzAccount -ServicePrincipal -TenantId $TenantId -Credential $Credential -WarningAction Ignore

# Get subscriptions
$subscriptions = Get-AzSubscription
Write-Host "Total subscriptions: $($subscriptions.Count)" -ForegroundColor Cyan

# Output paths
$currentDate = Get-Date -Format "yyyy-MM-dd"
$filename = "CIS_Benchmark_Windows2022_Baseline_1_0_$currentDate"
$csvFilePath = ".\sourcefiles\$filename.csv"
$jsonFilePath = ".\sourcefiles\$filename.json"

# Cleanup old files if they exist
if (Test-Path $csvFilePath) {
    Remove-Item -Path $csvFilePath -Force
}
if (Test-Path $jsonFilePath) {
    Remove-Item -Path $jsonFilePath -Force
}

# CSV Header
$csvHeader = "bunit,subscription,report_id,date,host_name,region,environment,platform,status,id,title"
Add-Content -Path $csvFilePath -Value $csvHeader

# Query per subscription
foreach ($sub in $subscriptions) {
    $subId = $sub.Id
    $subName = $sub.Name

    Write-Host "Processing subscription: $subName ($subId)" -ForegroundColor Cyan

    # Query all relevant VM guest configuration resources for this subscription
    $vmQuery = @"
guestconfigurationresources
| where subscriptionId == '$subId'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
"@

    $vmResults = Search-AzGraph -Query $vmQuery

    if ($vmResults.Count -eq 0) {
        continue
    }

    Write-Host "Found $($vmResults.Count) VMs in subscription $subName" -ForegroundColor Cyan

    foreach ($vm in $vmResults) {
        $vmId = $vm.vmid
        Write-Host "Fetching compliance report for VM: $vmId" -ForegroundColor Cyan

        # Query compliance details per VM
        $complianceQuery = @"
guestconfigurationresources
| where id == '$($vm.id)'
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| project 
    subscriptionId, 
    id, 
    name, 
    location, 
    resources = properties.latestAssignmentReport.resources, 
    vmid = split(properties.targetResourceId, '/')[(-1)],
    reportid = split(properties.latestReportId, '/')[(-1)], 
    reporttime = properties.lastComplianceStatusChecked
| extend resources = iff(isnull(resources[0]), dynamic([{}]), resources)
| mv-expand resources limit 1000
| extend reasons = resources.reasons
| extend reasons = iff(isnull(reasons[0]), dynamic([{}]), reasons)
| mv-expand reasons
| project 
    bunit = "azure", 
    subscription = subscriptionId, 
    report_id = reportid, 
    date = format_datetime(todatetime(reporttime), "yyyy-MM-dd"), 
    host_name = vmid,
    region = location, 
    environment = case(
        tolower(substring(vmid, 5, 1)) == "d", "dev",
        tolower(substring(vmid, 5, 1)) == "q", "qa",
        tolower(substring(vmid, 5, 1)) == "u", "uat",
        tolower(substring(vmid, 5, 1)) == "p", "prod",
        "UNKNOWN"
    ),
    platform = split(name, "_")[2], 
    status = iif(
        reasons.phrase contains "This control is in the waiver list", 
        "skipped", 
        iif(resources.complianceStatus == "true", "passed", "failed")
    ),
    id = split(resources.resourceID, "_")[3], 
    title = replace_string(tostring(resources.resourceId), "[WindowsControlTranslation]", "")
"@

        $complianceResults = Search-AzGraph -Query $complianceQuery -First 1000

        foreach ($ctl in $complianceResults) {
            $line = "$($ctl.bunit),$($ctl.subscription),$($ctl.report_id),$($ctl.date),$($ctl.host_name),$($ctl.region),$($ctl.environment),$($ctl.platform),$($ctl.status),$($ctl.id),$($ctl.title)"
            Add-Content -Path $csvFilePath -Value $line
        }
    }
}

Write-Host "All subscriptions processed"
Write-Host "Converting CSV to JSON..."

# Convert CSV to JSON
Import-Csv -Path $csvFilePath | ConvertTo-Json -Depth 10 | Set-Content -Path $jsonFilePath -Encoding utf8

Write-Host "Report generation completed"


---
$compliancequery = @"
guestconfigurationresources
| where type =~ 'microsoft.guestconfiguration/guestconfigurationassignments'
| where name contains 'CIS_Benchmark_Windows2022_Baseline_1_0'
| where subscriptionId == '$($sub.Id)'
| extend
    vmid = split(properties.targetResourceId, '/')[array_length(split(properties.targetResourceId, '/')) - 1],
    reportid = split(properties.latestReportId, '/')[array_length(split(properties.latestReportId, '/')) - 1],
    reporttime = properties.lastComplianceStatusChecked,
    resources = properties.latestAssignmentReport.resources
| mv-expand resources
| extend reasons = iif(isnull(resources.reasons), dynamic([]), resources.reasons)
| project
    bunit = 'azure',
    subscription = subscriptionId,
    report_id = reportid,
    report_date = format_datetime(todatetime(reporttime), 'yyyy-MM-dd'),
    host_name = vmid,
    region = location,
    environment = case(tolower(substring(vmid, 5, 1)) == 'd', 'dev',
                       tolower(substring(vmid, 5, 1)) == 'q', 'qa',
                       tolower(substring(vmid, 5, 1)) == 'u', 'uat',
                       tolower(substring(vmid, 5, 1)) == 'p', 'prod', 'UNKNOWN'),
    platform = split(name, '_')[2],
    status = iif(array_length(reasons) > 0 and reasons[0].phrase contains 'This control is in the waiver list', 'skipped',
                 iif(resources.complianceStatus == 'true', 'passed', 'failed')),
    id = split(resources.resourceId, '_')[3],
    title = replace_string(tostring(resources.resourceId), '[WindowsControlTranslation]', '')
"@


----
Hi Prabhu,

Thank you for getting back to me and for pointing me to the Confluence page with the GCS-approved CIS benchmark responses.

I had already reviewed this document as part of the analysis we’re doing for the waivers functionality. Specifically, we are automating the validation of Amazon Inspector 2 scan results against GCS-approved CIS benchmark profiles, with the goal of marking any already-approved findings as "Skipped"—regardless of their original status in Inspector 2—to reflect more accurate compliance reporting.

While analyzing the data for Amazon Linux 2023, I followed the approach mentioned in the Confluence file—filtering the “Response for Amazon Linux 2023” column for “NA.” However, I encountered discrepancies between the CIS IDs listed in the GCS document and those in the CIS Benchmark v1.0.0 used by Inspector 2.

For example:

The GCS document marks CIS ID 1.1.10 as approved.

However, in the actual Amazon Linux 2023 CIS profile used by Inspector 2, the rule with a matching description is CIS ID 1.1.3.1.

These mismatches in rule IDs, despite similar descriptions, make automated matching unreliable. I’ve documented these discrepancies on the Confluence page and initially raised this concern with Jack. After reviewing it, Jack confirmed that this issue needs to be addressed by the GCS team.

Additionally, for the other supported platforms (e.g., Amazon Linux 2, Windows Server 2019/2022, Rocky Linux 8, RHEL 8.8/8.10), the situation is more unclear:

There are multiple Excel files uploaded to Confluence for the same OS.

It's difficult to determine which file is the latest or most authoritative.

Several of these files lack the “Response” columns altogether, which were crucial in the Amazon Linux case for filtering approved rules.

Given these challenges, could you please help confirm:

Is there a single, reliable source of truth that contains the GCS-approved CIS IDs for each supported platform?

If so, could you share the latest and most accurate version of those datasets?

Having consistent and reliable data is critical for generating an updated waivers.yaml file that lists all approved waiver IDs per platform. This file directly feeds into our exemption processing logic, so accuracy here is essential for downstream systems.

I’d really appreciate your help in clarifying this.
---
Just wanted to check—were you provided the option to Squash and Merge while merging that PR?

Typically, using squash and merge helps keep the Git history clean. Ideally, after merging a PR, develop should reflect a single commit summarizing the feature work. However, I noticed that around 150 commits from the feature branch were merged in directly.

It’s nothing critical and won’t cause any issues, but I wanted to understand if there's a specific process being followed here 
---
Title:
Release Feature Branch to Master in chf_data_ingestion Following Standard Process

Description:
As a platform engineer, I need to release the code currently in a feature branch of the chf_data_ingestion repository,
so that the functionality can be made available in production as per our release workflow.

The release process requires the following steps:

Merge the feature branch into the develop branch.

Promote changes from develop to the release branch.

After approvals, merge the release branch into master.

Update the relevant Confluence page with release details, including scope, date, and commit references.

Acceptance Criteria:

The feature branch is successfully merged into develop with no conflicts.

Changes are promoted from develop to release and required approvals are obtained.

Final merge from release to master is completed following internal approval and quality gates.

Confluence release documentation is updated with:

Release date

Feature summary

Relevant branch names and commit hashes

All merges are traceable and follow the team's version control and tagging conventions.
---
Request GCS-Approved CIS Benchmark Data for Waivers Implementation

Description:
To implement waivers logic for Inspector 2 findings, we need an accurate and centralized list of GCS-approved CIS benchmark rules per platform. Current data in Confluence is inconsistent—CIS IDs do not match across sources, and multiple Excel files exist for the same platforms with no clear versioning or filtering criteria.

This is blocking automation efforts to generate an accurate waivers.yaml file.

Platforms:

amazon_linux_2023

amazon_linux_2

microsoft_windows_server_2022

microsoft_windows_server_2019

rocky_rocky_linux_8

redhat_redhat_enterprise_linux_8.10

redhat_redhat_enterprise_linux_8.8

Acceptance Criteria:

A single, verified source of GCS-approved CIS IDs is provided per platform.

The data includes CIS ID, rule description, approval status, and platform.

The CIS IDs align with those used in Amazon Inspector 2 scan outputs.

Format is Excel, CSV, or a centrally maintained reference document.


---
Subject: Request for Clear and Accurate GCS-Approved CIS Data for Waivers Implementation

Hi Jack,

I hope you're doing well.

As part of the waivers functionality we're building, we are automating the process of validating Amazon Inspector 2 scan results against the GCS-approved CIS benchmark profiles. The goal is to mark any findings already approved by GCS as Skipped, regardless of their original status in Inspector 2, to reflect more accurate compliance reporting.

The logic to implement this is already in place, but I’m currently facing challenges in sourcing reliable and consistent data on which CIS profiles have been approved by GCS for the supported platforms:

amazon_linux_2023

amazon_linux_2

microsoft_windows_server_2022

microsoft_windows_server_2019

rocky_rocky_linux_8

redhat_redhat_enterprise_linux_8.10

redhat_redhat_enterprise_linux_8.8

When I initially started the analysis with Amazon Linux 2023, you mentioned during our sync to filter the “Response for Amazon Linux 2023” column for “NA.” However, upon doing that and comparing the filtered entries to the actual CIS Benchmark v1.0.0 used by Inspector 2, I noticed several discrepancies.

For instance, GCS marks CIS ID 1.1.10 as approved, but in the Amazon CIS profile, the corresponding rule with the same description is 1.1.3.1. This mismatch in IDs while having matching descriptions makes the automated matching process unreliable. I have documented these discrepancies in the Confluence page.

Additionally, for the other platforms, there are multiple Excel files uploaded to Confluence for the same OS, with no clear indication of which one is the latest. Most of them also lack the "Response" columns used for filtering in the Amazon Linux case, making it difficult to extract a definitive list of GCS-approved CIS rules.

Given this situation, I would like to check:

Is there a single source of truth for the GCS-approved CIS IDs per platform?

If yes, could you please help point me to it or share the latest and most accurate data files?

Accurate data is critical for me to complete this task, which will output an updated waivers.yaml file listing all approved waiver IDs grouped by platform. This is essential to ensure downstream exemption processing is accurate.

Thanks so much for your help on this.

Best regards,
[Your Name]
----
# 📄 CHANGELOG

## [1.0.0] - 2025-05-12

### ✅ Added
- Initial project setup for `chf_data_ingestion`.
- Directory structure including `source/`, `output/`, `waivers.yaml`, and `playbooks/`.
- Ansible playbooks for each pipeline stage: `download.yml`, `transform.yml`, `waivers.yml`.
- Automated scheduling via Ansible Tower (Mondays at 09:00 AM London time).

### 🚀 New Features
- **Scan Report Download**: Connects to AWS Inspector 2 and pulls CIS benchmark scan reports in Excel format.
- **Data Transformation**: Cleans and converts Excel data to structured JSON.
- **Waiver Application**: Applies waivers based on `waivers.yaml` to mark findings as waived by platform and ID.
- **Splunk Integration**: Publishes the final JSON output to Splunk for compliance tracking.

### ✏️ Notable Changes
- Standardized JSON output format for interoperability with downstream tools.
- Introduced logical separation between raw, transformed, and waived data.

### ⚠️ Deprecation
- None in this release.

### ❌ Removals
- None in this release.


----
# 📦 chf_data_ingestion

`chf_data_ingestion` is an automated pipeline for retrieving, processing, and publishing AWS Inspector 2 scan reports. This system supports CIS compliance validation and integrates waiver handling based on a centrally maintained `waivers.yaml` file. The processed data is finally sent to Splunk for monitoring and reporting.

---

## 🔄 Workflow Summary

The pipeline is divided into three main stages, executed via **Ansible** tasks:

### 1. 🟡 Download Scan Reports
- Connects to **AWS Inspector 2**.
- Downloads the latest **CIS benchmark scan reports** in Excel format.
- Saves the files in the `source/` directory.

### 2. 🟢 Transform and Clean Data
- Reads Excel files from `source/`.
- Cleans and converts them into structured **JSON files**.
- Outputs the result to the `output/` directory.

### 3. 🔵 Apply Waivers
- Reads the cleaned JSON data.
- Cross-references findings with waiver rules in `waivers.yaml`.
- Marks matching entries as **waived** based on platform and waiver ID.

### 4. 🔁 Publish to Splunk
- Transformed and waiver-processed JSON data is sent to **Splunk** for centralized compliance tracking.

---

## ⏰ Automation & Scheduling

- Each stage is triggered as an **Ansible task**:
  - `download`
  - `transform`
  - `waivers`

- The tasks are scheduled in **Ansible Tower** to run automatically:

  > **Schedule:** Every **Monday at 09:00 AM (London time)**

---

## 📁 Folder Structure

```bash
chf_data_ingestion/
├── source/         # Raw Excel scan reports from AWS Inspector
├── output/         # Cleaned and transformed JSON data
├── waivers.yaml    # Waiver definitions by platform and ID
├── playbooks/
│   ├── download.yml
│   ├── transform.yml
│   └── waivers.yml
├── README.md       # Project documentation



----
I need to validate Amazon Inspector 2 scan results against GCS-approved CIS benchmark profiles,
so that I can update waivers.yaml with the correct waiver IDs for approved rules across supported platforms.

Description
The task involves automating the comparison between scan results from Amazon Inspector 2 (which follows CIS benchmarks) and an Excel list of approved CIS rules provided by the GCS (Governance, Compliance & Security) team. The goal is to identify which findings from Inspector 2 are approved by GCS and should therefore be added to waivers.yaml under the corresponding platform.

Supported platforms include:

amazon_linux_2023

amazon_linux_2

microsoft_windows_server_2022

microsoft_windows_server_2019

rocky_rocky_linux_8

The final output is an updated waivers.yaml file that lists all approved waiver_ids under the correct platform, so they can be processed for exemption downstream.

Acceptance Criteria
Input Handling:

The script accepts:

Amazon Inspector 2 scan output (in JSON format).

GCS-approved profile list (in Excel or CSV format).

Existing waivers.yaml file.

Data Comparison:

The script matches CIS benchmark IDs (or equivalent identifiers) between the Inspector 2 scan and GCS list.

Only exact matches (by ID) are considered approved.

Platform-Specific Logic:

The comparison is scoped to the five supported platforms.

Waivers are grouped and written under their respective platform names in waivers.yaml.

YAML Update:

If waivers.yaml exists, it is updated in-place with new waiver IDs (duplicates are avoided).

If it doesn't exist, the script creates a new file with correct structure.

Validation:

The script logs:

Total number of findings scanned.

Number of approved waivers added per platform.

Any findings that did not match an approved profile.

Error Handling:

Graceful handling of malformed files, unsupported platforms, or mismatched formats.

Alerts/logs are generated for any inconsistencies.
---
Description
I need to write a Python script that reads waiver IDs from a waivers.yaml file and updates the status of matching entries in a list of JSON files,
so that the JSON files reflect which waiver checks should be marked as "waived" for specific platforms.

Acceptance Criteria
YAML Parsing:

The script correctly reads and parses waivers.yaml.

The YAML file contains platforms and lists of waiver IDs under each platform.

JSON Processing:

The script reads all .json files from a specified directory.

Each JSON file contains a list or dictionary of items, each with at least waiver_id, status, and platform fields.

Matching Logic:

For each JSON entry, if the platform matches an entry in waivers.yaml, and the waiver_id is in the corresponding waiver_id list, then update its status to "waived".

File Update:

The modified JSON data is saved back to the file or to a new file (based on configuration or prompt).

No JSON files are updated if there are no matches.

Logging & Output:

The script logs the number of updated entries per file.

Errors in reading or writing files are logged gracefully.

Error Handling:

Proper error handling is in place for missing fields, malformed JSON, or YAML syntax errors.

Test Case:

A test directory with sample JSON files and a sample waivers.yaml can be processed without errors and produces correct output.

----
import os
import json
import yaml

# Load waivers.yaml
with open("waivers.yaml", "r") as f:
    waivers = yaml.safe_load(f)

waiver_map = {}
for platform, items in waivers.items():
    for item in items:
        platform_name = item["platform"]
        waiver_ids = item.get("waivers_id", [])
        waiver_map[platform_name] = set(waiver_ids)

# Directory containing JSON files
source_dir = "sourcefiles"

# Process each JSON file
for filename in os.listdir(source_dir):
    if filename.endswith(".json"):
        filepath = os.path.join(source_dir, filename)

        with open(filepath, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                print(f"Skipping invalid JSON: {filename}")
                continue

        # Check if it's a list of objects
        if isinstance(data, list):
            modified = False
            for item in data:
                platform = item.get("platform")
                check_id = item.get("id")
                if platform in waiver_map and check_id in waiver_map[platform]:
                    if item.get("status") != "skipped":
                        item["status"] = "skipped"
                        modified = True
            if modified:
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2)
                print(f"Updated entries in: {filename}")
        else:
            print(f"Unexpected JSON format (not a list) in: {filename}")



---------------
# Calculate pipeline_name: Remove -gl if it ends with -gl
      pipeline_name = endswith(each.value.repo_name, "-gl") ? substr(each.value.repo_name, 0, length(each.value.repo_name) - 3) : each.value.repo_name

      # Calculate pipeline_identifier: Replace - with space and remove -gl if it ends with -gl
      pipeline_identifier = endswith(each.value.repo_name, "-gl") ? 
        replace(substr(each.value.repo_name, 0, length(each.value.repo_name) - 3), "-", " ") : 
        replace(each.value.repo_name, "-", " ")
---
#!/bin/bash

# Variables
VIRTUALSERVICE_NAME="vs-state-mgmt-canary-svc-svc"
NAMESPACE="<namespace>"  # Replace with your namespace

echo "Fetching VirtualService $VIRTUALSERVICE_NAME from namespace $NAMESPACE..."

# Get the VirtualService JSON
virtual_service=$(oc get virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE -o json)

if [ $? -ne 0 ]; then
  echo "Failed to fetch VirtualService. Please check the name and namespace."
  exit 1
fi

echo "Successfully fetched VirtualService. Now checking for route with 'reg-channel-version: ocp-svc-test'..."

# Find the index of the route with the ocp-svc-test header
route_index=$(echo $virtual_service | jq -r '.spec.http | to_entries[] | select(.value.match[].headers["reg-channel-version"].exact == "ocp-svc-test") | .key')

# Debugging: Print the full http section for easier visibility
echo "HTTP section of the VirtualService:"
echo $virtual_service | jq '.spec.http'

# Check if the route exists
if [ -z "$route_index" ]; then
  echo "No route found with 'reg-channel-version: ocp-svc-test' in VirtualService $VIRTUALSERVICE_NAME."
  exit 0  # Exit without error if the route doesn't exist
else
  echo "Route with 'reg-channel-version: ocp-svc-test' found at index $route_index."
  
  # Debugging: Print the exact match that will be deleted
  echo "The following route will be deleted:"
  echo $virtual_service | jq ".spec.http[$route_index]"

  # Delete the route using the identified index
  echo "Proceeding to delete the route..."
  oc patch virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE \
    --type='json' \
    -p="[{\"op\": \"remove\", \"path\": \"/spec/http/$route_index\"}]"

  if [ $? -eq 0 ]; then
    echo "Route with 'reg-channel-version: ocp-svc-test' successfully deleted from VirtualService $VIRTUALSERVICE_NAME."
  else
    echo "Failed to delete the route. Please check the output above for more details."
  fi
fi

---
#!/bin/bash

# Variables
VIRTUALSERVICE_NAME="vs-state-mgmt-canary-svc-svc"
NAMESPACE="<namespace>"  # Replace with the correct namespace

# Get the VirtualService JSON
virtual_service=$(oc get virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE -o json)

# Find the index of the route with the ocp-gc header
route_index=$(echo $virtual_service | jq -r '.spec.http | to_entries[] | select(.value.match[].headers["reg-channel-version"].exact == "ocp-gc") | .key')

# Check if the route exists
if [ -z "$route_index" ]; then
  echo "Route with 'reg-channel-version: ocp-gc' header not found in VirtualService $VIRTUALSERVICE_NAME."
  exit 1
fi

# Delete the route using the identified index
oc patch virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE \
  --type='json' \
  -p="[{\"op\": \"remove\", \"path\": \"/spec/http/$route_index\"}]"

echo "Route with 'reg-channel-version: ocp-gc' header deleted from VirtualService $VIRTUALSERVICE_NAME."

---
oc patch virtualservice vs-state-mgmt-canary-svc-svc -n default --type=json -p='[{"op": "add", "path": "/spec/http/0", "value": {"match": [{"headers": {"reg-channel-version": {"exact": "ocp-gc"}}}, {"uri": {"prefix": "/core/state-mgmt-service/v1"}}], "route": [{"destination": {"host": "iau-state-mgmt-core-service", "port": {"number": 8080}, "subset": "iau-state-mgmt-core-service-ocp-gc"}}]}}]'

---
apiVersion: batch/v1
kind: Job
metadata:
  name: remove-matcher-job
  namespace: default  # Change this if needed
  annotations:
    argocd.argoproj.io/hook: PostDelete
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      containers:
      - name: remove-matcher
        image: bitnami/kubectl:latest  # Using a lightweight image with kubectl
        command: ["/bin/bash"]
        args:
        - -c
        - |
          #!/bin/bash

          # Define variables
          VIRTUALSERVICE_NAME="vs-state-mgmt-canary-svc-svc"
          NAMESPACE="default"
          SUBSET_NAME="iau-state-mgmt-core-service-ocp-gc"

          # Get the current VirtualService configuration
          MATCHER_INDEX=$(kubectl get virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE -o jsonpath='{range .spec.http[*]}{.route[*].destination.subset}{"\n"}{end}' | grep -n $SUBSET_NAME | cut -d: -f1)

          # Check if the matcher exists
          if [[ -n $MATCHER_INDEX ]]; then
              # Adjust index to match JSON path
              JSON_INDEX=$((MATCHER_INDEX - 1))

              # Remove the matcher
              kubectl patch virtualservice $VIRTUALSERVICE_NAME -n $NAMESPACE --type=json -p="[{'op': 'remove', 'path': '/spec/http/$JSON_INDEX'}]"
              echo "Matcher with subset '$SUBSET_NAME' has been removed."
          else
              echo "No matcher found with subset '$SUBSET_NAME'."
          fi
      restartPolicy: Never


---
openssl crl2pkcs7 -nocrl -certfile ca.crt | openssl pkcs7 -print_certs -text -noout

---

 def branchExists = sh(
                            script: """
                            curl -s -o /dev/null -w "%{http_code}" -H "Authorization: token ${GITHUB_TOKEN}" \
                            https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/branches/${BRANCH_NAME}
                            """,
                            returnStatus: true
                        ) == 200

---
#!/bin/bash

# Actuator health endpoint (assuming it's exposed via a service)
ACTUATOR_ENDPOINT="http://your-canary-service/actuator/health"

# Polling interval (in seconds)
POLL_INTERVAL=30

# Maximum runtime (in seconds)
MAX_RUNTIME=900

# Number of requests to send in each batch
REQUEST_BATCH_SIZE=10

# Number of batches to send
TOTAL_BATCHES=5

# Function to check actuator health
check_actuator_health() {
  RESPONSE=$(curl -s $ACTUATOR_ENDPOINT)
  STATUS=$(echo $RESPONSE | grep -o '"status":"[^"]*"' | grep -o '[^"]*$')
  
  if [ "$STATUS" == "UP" ]; then
    return 0
  else
    return 1
  fi
}

# Polling loop with timer
start_time=$(date +%s)

while true; do
  echo "Checking actuator health..."
  
  if check_actuator_health; then
    echo "Actuator is up and healthy. Sending additional requests to confirm..."

    # Initialize the counter for successful batches
    successful_batches=0

    # Send additional requests in batches
    for ((batch=1; batch<=TOTAL_BATCHES; batch++)); do
      batch_successful=true

      for ((req=1; req<=REQUEST_BATCH_SIZE; req++)); do
        if ! check_actuator_health; then
          echo "Request $req in batch $batch failed. Resetting counter."
          batch_successful=false
          break
        fi
      done

      if $batch_successful; then
        echo "Batch $batch successful."
        successful_batches=$((successful_batches + 1))
      else
        successful_batches=0
        break
      fi

      # Sleep between batches if there are more batches to send
      if [ $batch -lt $TOTAL_BATCHES ]; then
        echo "Waiting $POLL_INTERVAL seconds before next batch..."
        sleep $POLL_INTERVAL
      fi
    done

    # Check if all batches were successful
    if [ $successful_batches -eq $TOTAL_BATCHES ]; then
      echo "All batches successful. Proceeding with BDD tests."
      break
    else
      echo "Some batches failed. Restarting the check."
    fi
  else
    echo "Actuator is not up yet. Waiting..."
    sleep $POLL_INTERVAL
  fi
  
  # Check elapsed time
  current_time=$(date +%s)
  elapsed_time=$((current_time - start_time))
  
  if [ $elapsed_time -ge $MAX_RUNTIME ]; then
    echo "Maximum runtime exceeded. Exiting..."
    exit 1
  fi
done

exit 0

----
post {
        success {
            script {
                // Fetch the latest commit SHA for the PR
                def response = sh(script: "curl -s -H \"Authorization: token ${env.GITHUB_TOKEN}\" https://api.github.com/repos/${env.REPO_OWNER}/${env.REPO_NAME}/pulls/${env.PR_NUMBER}", returnStdout: true).trim()
                def json = readJSON text: response
                def commitSha = json.head.sha
                
                // Update the status for the commit
                def statusPayload = """{
                    "state": "success",
                    "description": "Build succeeded",
                    "context": "continuous-integration/jenkins"
                }"""
                sh "curl -X POST -H \"Authorization: token ${env.GITHUB_TOKEN}\" -d '${statusPayload}' https://api.github.com/repos/${env.REPO_OWNER}/${env.REPO_NAME}/statuses/${commitSha}"
            }
        }
        failure {
            script {
                // Fetch the latest commit SHA for the PR
                def response = sh(script: "curl -s -H \"Authorization: token ${env.GITHUB_TOKEN}\" https://api.github.com/repos/${env.REPO_OWNER}/${env.REPO_NAME}/pulls/${env.PR_NUMBER}", returnStdout: true).trim()
                def json = readJSON text: response
                def commitSha = json.head.sha
                
                // Update the status for the commit
                def statusPayload = """{
                    "state": "failure",
                    "description": "Build failed",
                    "context": "continuous-integration/jenkins"
                }"""
                sh "curl -X POST -H \"Authorization: token ${env.GITHUB_TOKEN}\" -d '${statusPayload}' https://api.github.com/repos/${env.REPO_OWNER}/${env.REPO_NAME}/statuses/${commitSha}"
            }
        }
    }
}

---
pipeline {
    agent any
    environment {
        GITHUB_TOKEN = credentials('github-token-id') // GitHub token stored in Jenkins credentials
    }
    stages {
        stage('Fetch PR Labels') {
            steps {
                script {
                    def prNumber = env.ghprbPullId
                    def repo = 'owner/repository'
                    def response = sh(script: "curl -s -H \"Authorization: token ${env.GITHUB_TOKEN}\" -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/repos/${repo}/issues/${prNumber}/labels", returnStdout: true).trim()
                    def labels = readJSON(text: response)
                    def appName = labels.find { it.name.startsWith('appname:') }?.name?.split(': ')[1]
                    def releaseName = labels.find { it.name.startsWith('releaseName:') }?.name?.split(': ')[1]
                    
                    if (!appName || !releaseName) {
                        error("Required labels not found.")
                    }
                    
                    env.ACTUATOR_ENDPOINT = "https://sitsasasasaasas/${appName}/v1.0/actuator/health"
                    env.RELEASE_NAME = releaseName
                    
                    echo "Labels for PR #${prNumber}: ${labels*.name.join(', ')}"
                    echo "ACTUATOR_ENDPOINT: ${env.ACTUATOR_ENDPOINT}"
                    echo "RELEASE_NAME: ${env.RELEASE_NAME}"
                }
            }
        }
        stage('Check Actuator Health') {
            steps {
                withCredentials([
                    file(credentialsId: 'tls-crt-id', variable: 'TLS_CRT_PATH'),
                    file(credentialsId: 'tls-key-id', variable: 'TLS_KEY_PATH')
                ]) {
                    sh """
                    chmod +x path/to/your/script.sh
                    path/to/your/script.sh \$TLS_CRT_PATH \$TLS_KEY_PATH \$ACTUATOR_ENDPOINT \$RELEASE_NAME
                    """
                }
            }
        }
        // Other stages...
    }
}

---
pipeline {
    agent any
    environment {
        GITHUB_TOKEN = credentials('github-token-id') // GitHub token stored in Jenkins credentials
    }
    stages {
        stage('Fetch PR Labels') {
            steps {
                script {
                    def prNumber = env.ghprbPullId
                    def repo = 'owner/repository'
                    def response = sh(script: "curl -s -H \"Authorization: token ${env.GITHUB_TOKEN}\" -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/repos/${repo}/issues/${prNumber}/labels", returnStdout: true).trim()
                    def labels = readJSON(text: response)
                    echo "Labels for PR #${prNumber}: ${labels*.name.join(', ')}"
                }
            }
        }
        // Other stages...
    }
}

---------------------
#!/bin/bash

# Actuator health endpoint (assuming it's exposed via a service)
ACTUATOR_ENDPOINT="http://your-canary-service/actuator/health"

# Polling interval (in seconds)
POLL_INTERVAL=30

# Maximum runtime (in seconds)
MAX_RUNTIME=900

# Function to check actuator health
check_actuator_health() {
  RESPONSE=$(curl -s $ACTUATOR_ENDPOINT)
  STATUS=$(echo $RESPONSE | grep -o '"status":"[^"]*"' | grep -o '[^"]*$')
  
  if [ "$STATUS" == "UP" ]; then
    return 0
  else
    return 1
  fi
}

# Polling loop with timer
start_time=$(date +%s)

while true; do
  echo "Checking actuator health..."
  
  if check_actuator_health; then
    echo "Actuator is up and healthy."
    break
  else
    echo "Actuator is not up yet. Waiting..."
    sleep $POLL_INTERVAL
  fi
  
  # Check elapsed time
  current_time=$(date +%s)
  elapsed_time=$((current_time - start_time))
  
  if [ $elapsed_time -ge $MAX_RUNTIME ]; then
    echo "Maximum runtime exceeded. Exiting..."
    exit 1
  fi
done

echo "Canary instance is healthy. Proceeding with BDD tests."
exit 0

----------------------

pipeline {
    agent any
    environment {
        ACTUATOR_ENDPOINT = "https://your-canary-service/actuator/health"
    }
    stages {
        stage('Wait for Canary Pods') {
            steps {
                script {
                    // Fetch the certificate and key from Jenkins credentials
                    withCredentials([string(credentialsId: 'SIT_TLS_CRT', variable: 'CERT_CONTENT'),
                                     string(credentialsId: 'SIT_TLS_KEY', variable: 'KEY_CONTENT')]) {

                        // Write the certificate and key to the specified paths
                        writeFile file: 'path/tls.crt', text: CERT_CONTENT
                        writeFile file: 'path/tls.key', text: KEY_CONTENT

                        def pollScript = """
                        #!/bin/bash
                        ACTUATOR_ENDPOINT="${env.ACTUATOR_ENDPOINT}"
                        CERT_PATH="path/tls.crt"
                        KEY_PATH="path/tls.key"
                        POLL_INTERVAL=30

                        check_actuator_health() {
                            echo "Polling Actuator Endpoint: \$ACTUATOR_ENDPOINT"
                            CURL_COMMAND="curl -s --cert \$CERT_PATH --key \$KEY_PATH \$ACTUATOR_ENDPOINT"
                            echo "Formed Request: \$CURL_COMMAND"
                            RESPONSE=\$(\$CURL_COMMAND)
                            STATUS=\$(echo \$RESPONSE | grep -o '"status":"[^"]*"' | grep -o '[^"]*\$')
                            echo "Response: \$RESPONSE"
                            echo "Parsed Status: \$STATUS"
                            if [ "\$STATUS" == "UP" ]; then
                                return 0
                            else
                                return 1
                            fi
                        }

                        while true; do
                            echo "Checking actuator health..."
                            if check_actuator_health; then
                                echo "Actuator is up and healthy."
                                break
                            else
                                echo "Actuator is not up yet. Waiting..."
                                sleep \$POLL_INTERVAL
                            fi
                        done

                        echo "Canary instance is healthy. Proceeding with BDD tests."
                        exit 0
                        """
                        writeFile file: 'pollActuatorHealth.sh', text: pollScript
                        sh 'chmod +x pollActuatorHealth.sh'
                        sh './pollActuatorHealth.sh'
                    }
                }
            }
        }
        stage('Run BDD Tests') {
            steps {
                echo 'Running BDD tests...'
                // Add your steps to run BDD tests here
            }
        }
    }
}

----

#!/bin/bash
ACTUATOR_ENDPOINT="$1"
CERT_PATH="$2"
KEY_PATH="$3"
POLL_INTERVAL=30

check_actuator_health() {
    echo "Polling Actuator Endpoint: $ACTUATOR_ENDPOINT"
    CURL_COMMAND="curl -s --cert $CERT_PATH --key $KEY_PATH $ACTUATOR_ENDPOINT"
    echo "Formed Request: $CURL_COMMAND"
    RESPONSE=$($CURL_COMMAND)
    STATUS=$(echo $RESPONSE | grep -o '"status":"[^"]*"' | grep -o '[^"]*$')
    echo "Response: $RESPONSE"
    echo "Parsed Status: $STATUS"
    if [ "$STATUS" == "UP" ]; then
        return 0
    else
        return 1
    fi
}

while true; do
    echo "Checking actuator health..."
    if check_actuator_health; then
        echo "Actuator is up and healthy."
        break
    else
        echo "Actuator is not up yet. Waiting..."
        sleep $POLL_INTERVAL
    fi
done

echo "Canary instance is healthy. Proceeding with BDD tests."
exit 0

pipeline {
    agent any
    environment {
        ACTUATOR_ENDPOINT = "https://your-canary-service/actuator/health"
    }
    stages {
        stage('Wait for Canary Pods') {
            steps {
                script {
                    // Fetch the certificate and key from Jenkins credentials
                    withCredentials([string(credentialsId: 'SIT_TLS_CRT', variable: 'CERT_CONTENT'),
                                     string(credentialsId: 'SIT_TLS_KEY', variable: 'KEY_CONTENT')]) {

                        // Write the certificate and key to the specified paths
                        writeFile file: 'path/tls.crt', text: CERT_CONTENT
                        writeFile file: 'path/tls.key', text: KEY_CONTENT

                        // Write the poll script to the workspace
                        writeFile file: 'pollActuatorHealth.sh', text: libraryResource('path/to/pollActuatorHealth.sh')
                        sh 'chmod +x pollActuatorHealth.sh'

                        // Execute the poll script
                        sh './pollActuatorHealth.sh "${env.ACTUATOR_ENDPOINT}" "path/tls.crt" "path/tls.key"'
                    }
                }
            }
        }
        stage('Run BDD Tests') {
            steps {
                echo 'Running BDD tests...'
                // Add your steps to run BDD tests here
            }
        }
    }
}


----

pipeline {
    agent any
    environment {
        GITHUB_TOKEN = credentials('github-token-id') // GitHub token stored in Jenkins credentials
    }
    stages {
        stage('Fetch PR Labels') {
            steps {
                script {
                    def prNumber = env.ghprbPullId
                    def repo = 'owner/repository'
                    def response = sh(script: "curl -s -H \"Authorization: token ${env.GITHUB_TOKEN}\" -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/repos/${repo}/issues/${prNumber}/labels", returnStdout: true).trim()
                    def labels = readJSON(text: response)
                    
                    // Ensure there are exactly 4 labels
                    if (labels.size() != 4) {
                        error("Expected exactly 4 labels, but found ${labels.size()}.")
                    }
                    
                    // Extract appName and releaseName from labels
                    def appNameLabel = labels.find { it.name.startsWith('appname:') }
                    def releaseNameLabel = labels.find { it.name.startsWith('releaseName:') }
                    
                    if (!appNameLabel || !releaseNameLabel) {
                        error("Required labels not found.")
                    }
                    
                    def appName = appNameLabel.name.split(': ')[1]
                    def releaseName = releaseNameLabel.name.split(': ')[1]
                    
                    // Read the YAML file
                    def appConfig = readYaml file: 'path/to/app_config.yaml'
                    def serviceConfig = appConfig.services[appName]
                    
                    // Check if appName exists in YAML
                    if (!serviceConfig) {
                        error("Appname '${appName}' not found in YAML configuration. Stopping pipeline execution.")
                    }
                    
                    def contextPath = serviceConfig.context_path
                    
                    env.ACTUATOR_ENDPOINT = "https://your-canary-service${contextPath}"
                    env.RELEASE_NAME = releaseName
                    
                    echo "Labels for PR #${prNumber}: ${labels*.name.join(', ')}"
                    echo "ACTUATOR_ENDPOINT: ${env.ACTUATOR_ENDPOINT}"
                    echo "RELEASE_NAME: ${env.RELEASE_NAME}"
                }
            }
        }
        stage('Check Actuator Health') {
            when {
                expression { return env.ACTUATOR_ENDPOINT != null }
            }
            steps {
                withCredentials([
                    file(credentialsId: 'tls-crt-id', variable: 'TLS_CRT_PATH'),
                    file(credentialsId: 'tls-key-id', variable: 'TLS_KEY_PATH')
                ]) {
                    sh """
                    chmod +x path/to/your/script.sh
                    path/to/your/script.sh \$TLS_CRT_PATH \$TLS_KEY_PATH \$ACTUATOR_ENDPOINT \$RELEASE_NAME
                    """
                }
            }
        }
        // Other stages...
    }
}



